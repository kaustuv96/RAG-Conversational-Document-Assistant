{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb24e5e9-21d2-433a-94be-4e6a2943119f",
   "metadata": {},
   "source": [
    "#### Objectives\n",
    "\n",
    "1. Develop an AI-Powered Document Q&A Agent with Conversational Memory using RAG.\n",
    "2. Allow users to upload a PDF, ask questions, and receive answers grounded in the document.\n",
    "3. Maintain conversation history for follow-up questions.\n",
    "4. Use Gemini, LangChain, ChromaDB, and Streamlit.\n",
    "5. Show retrieved context used for grounding the answer.\n",
    "6. Handle API Key loading securely and track token usage (estimate per turn)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa42dfc7-234b-4c24-a329-eff8be689571",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc87a38c-2d1d-49b3-bb41-4ac584f48bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "import langchain\n",
    "import chromadb # Keep the import since Chroma is used\n",
    "import PyPDF2\n",
    "from io import BytesIO\n",
    "\n",
    "# LangChain specific imports\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate # Optional: for custom prompts if needed\n",
    "from langchain.schema import HumanMessage, AIMessage # To structure chat history for display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9740d8b7-b800-4aa5-9a90-09c7e93c35ca",
   "metadata": {},
   "source": [
    "#### Tokenizer for Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dad3b71e-a5c5-445c-aa12-34e3858bec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, logging as hf_logging\n",
    "# Suppress tokenizer warnings if needed\n",
    "hf_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4a9c9e-4573-4672-a05f-4537ded51042",
   "metadata": {},
   "source": [
    "#### Printing the Versions of Libraries Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "710be680-73b8-4134-90b2-90311da17880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries used:\n",
      "- streamlit: 1.37.1\n",
      "- langchain: 0.3.20\n",
      "- chromadb: 1.0.8\n",
      "- PyPDF2: 3.0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Libraries used:\")\n",
    "print(f\"- streamlit: {st.__version__}\")\n",
    "print(f\"- langchain: {langchain.__version__}\")\n",
    "print(f\"- chromadb: {chromadb.__version__}\")\n",
    "print(f\"- PyPDF2: {PyPDF2.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afba7a24-ffe4-4f51-be13-21f84fc73e40",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3a534e-f23b-4102-a17c-49abfd70eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "GEMINI_MODEL_NAME = \"gemini-2.0-flash\"\n",
    "EMBEDDING_MODEL_NAME = \"models/embedding-001\" # Standard Gemini embedding model\n",
    "# EMBEDDING_MODEL_NAME = \"gemini-embedding-exp-03-07\" # Standard Gemini embedding model\n",
    "\n",
    "# --- API Key Handling ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    st.error(\"ðŸ”´ Google API Key is missing! Please set the GOOGLE_API_KEY environment variable.\")\n",
    "    # ... (rest of the API key instructions - same as before) ...\n",
    "    st.stop()\n",
    "\n",
    "# --- Initialize Tokenizer ---\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\", legacy=False)\n",
    "except Exception as e:\n",
    "    st.warning(f\"âš ï¸ Failed to load tokenizer: {e}. Token counts will be estimated based on word count.\")\n",
    "    tokenizer = None\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Counts tokens using the loaded tokenizer or estimates with word count.\"\"\"\n",
    "    # ... (same count_tokens function as before) ...\n",
    "    if not text: return 0\n",
    "    if tokenizer:\n",
    "        try: return len(tokenizer.encode(text))\n",
    "        except Exception: return len(text.split())\n",
    "    else: return len(text.split())\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def extract_text_from_pdf(pdf_file_bytes):\n",
    "    \"\"\"Extracts text from PDF file bytes.\"\"\"\n",
    "    # ... (same extract_text_from_pdf function as before) ...\n",
    "    try:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file_bytes)\n",
    "        text = \"\"\n",
    "        for page in pdf_reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text: text += page_text + \"\\n\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error reading PDF file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38ffe29-183c-4e15-8ba7-63bb1961c24c",
   "metadata": {},
   "source": [
    "#### Setting up Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f66d58-a691-424b-b14b-08ae67700648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Streamlit Session State Initialization ---\n",
    "# To store state across reruns\n",
    "if 'vector_store' not in st.session_state:\n",
    "    st.session_state.vector_store = None\n",
    "if 'conversation_chain' not in st.session_state:\n",
    "    st.session_state.conversation_chain = None\n",
    "if 'chat_history_display' not in st.session_state: # Separate history for UI display\n",
    "    st.session_state.chat_history_display = []\n",
    "if 'processing_done' not in st.session_state:\n",
    "    st.session_state.processing_done = False\n",
    "# Token tracking (cumulative per session)\n",
    "if 'cumulative_input_tokens' not in st.session_state:\n",
    "    st.session_state.cumulative_input_tokens = 0\n",
    "if 'cumulative_output_tokens' not in st.session_state:\n",
    "    st.session_state.cumulative_output_tokens = 0\n",
    "\n",
    "def update_token_display():\n",
    "    \"\"\"Updates cumulative token usage display.\"\"\"\n",
    "    st.sidebar.markdown(\"### Token Usage (Cumulative Estimate)\")\n",
    "    # Simplified: Tracks Q+A tokens, not full context/history tokens sent to LLM\n",
    "    st.sidebar.markdown(f\"\"\"\n",
    "    **Input Tokens (Questions):** {st.session_state.cumulative_input_tokens:,}\n",
    "    **Output Tokens (Answers):** {st.session_state.cumulative_output_tokens:,}\n",
    "    **Total Tokens (Q+A):** {st.session_state.cumulative_input_tokens + st.session_state.cumulative_output_tokens:,}\n",
    "    \"\"\")\n",
    "    st.sidebar.caption(\"Note: Input tokens estimate user questions only, not the full context sent to the LLM.\")\n",
    "\n",
    "# ### Streamlit App Layout ---\n",
    "st.set_page_config(page_title=\"Conversational Document Q&A (RAG)\", layout=\"wide\")\n",
    "st.title(\"ðŸ’¬ Conversational AI Document Q&A with RAG\")\n",
    "st.markdown(f\"Upload a PDF, ask questions, and get answers based on the document's content. Powered by Gemini (`{GEMINI_MODEL_NAME}`), LangChain, Chroma, & Streamlit.\")\n",
    "\n",
    "# --- Sidebar ---\n",
    "with st.sidebar:\n",
    "    st.header(\"ðŸ“„ Document Upload\")\n",
    "    uploaded_file = st.file_uploader(\"Choose a PDF file\", type=\"pdf\")\n",
    "    process_button = st.button(\"Process Document\")\n",
    "    st.markdown(\"---\")\n",
    "    # Display token counts\n",
    "    update_token_display()\n",
    "    st.markdown(\"---\")\n",
    "    st.info(f\"\"\"\n",
    "    **Model:** {GEMINI_MODEL_NAME}\n",
    "    **Embeddings:** {EMBEDDING_MODEL_NAME}\n",
    "    **DB:** Chroma (In-Memory)\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47112ee1-689b-40d2-86de-048271e971e6",
   "metadata": {},
   "source": [
    "#### Processing Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab28ba7f-b3c2-4e71-8c98-b3a9d5552bbb",
   "metadata": {},
   "source": [
    "1. Document Processing Logic (only runs when button clicked and file uploaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01576a31-3859-4cf4-9e15-64f03c46ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Area ---\n",
    "if process_button and uploaded_file is not None:\n",
    "    with st.spinner(\"Processing document... This may take a few moments.\"):\n",
    "        try:\n",
    "            file_bytes = BytesIO(uploaded_file.read())\n",
    "            st.info(f\"ðŸ“„ Processing PDF: {uploaded_file.name}\")\n",
    "\n",
    "            # a) Extract Text\n",
    "            raw_text = extract_text_from_pdf(file_bytes)\n",
    "            if not raw_text:\n",
    "                st.error(\"Failed to extract text from PDF.\")\n",
    "                st.stop()\n",
    "\n",
    "            # b) Split Text\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1200, # Adjusted chunk size\n",
    "                chunk_overlap=150, # Adjusted overlap\n",
    "                length_function=len\n",
    "            )\n",
    "            texts = text_splitter.split_text(raw_text)\n",
    "            if not texts:\n",
    "                st.error(\"Failed to split document text.\")\n",
    "                st.stop()\n",
    "\n",
    "            # c) Create Embeddings\n",
    "            embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL_NAME, google_api_key=GOOGLE_API_KEY)\n",
    "            \n",
    "            # d) Create Vector Store (ChromaDB In-Memory)\n",
    "            st.session_state.vector_store = Chroma.from_texts(\n",
    "                texts=texts,\n",
    "                embedding=embeddings,\n",
    "                # Optional: Add source metadata if splitting preserves page numbers etc.\n",
    "                # metadatas=[{\"source\": f\"{uploaded_file.name}-chunk-{i}\"} for i in range(len(texts))]\n",
    "            )\n",
    "            retriever = st.session_state.vector_store.as_retriever(search_kwargs={\"k\": 5}) # Retrieve top 5 chunks\n",
    "\n",
    "            # e) Create Memory\n",
    "            # output_key='answer' ensures memory correctly captures the AI response\n",
    "            memory = ConversationBufferMemory(\n",
    "                memory_key='chat_history',\n",
    "                return_messages=True,\n",
    "                output_key='answer' # Crucial for ConversationalRetrievalChain\n",
    "            )\n",
    "\n",
    "            # f) Create LLM\n",
    "            llm = ChatGoogleGenerativeAI(\n",
    "                model=GEMINI_MODEL_NAME,\n",
    "                google_api_key=GOOGLE_API_KEY,\n",
    "                temperature=0.3, # Lower temperature for more factual Q&A\n",
    "                convert_system_message_to_human=True # Often needed for Gemini compatibility\n",
    "            )\n",
    "\n",
    "            # g) Create Conversational Retrieval Chain\n",
    "            st.session_state.conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "                llm=llm,\n",
    "                retriever=retriever,\n",
    "                memory=memory,\n",
    "                return_source_documents=True, # <<< Important to get sources\n",
    "                output_key='answer'           # <<< Explicitly define output key\n",
    "                # You can add custom prompts here if needed using `combine_docs_chain_kwargs`\n",
    "            )\n",
    "\n",
    "            st.session_state.processing_done = True\n",
    "            st.session_state.chat_history_display = [] # Reset display history for new doc\n",
    "            st.session_state.cumulative_input_tokens = 0 # Reset token counts\n",
    "            st.session_state.cumulative_output_tokens = 0\n",
    "            update_token_display() # Update sidebar counts\n",
    "            st.success(f\"âœ… Document '{uploaded_file.name}' processed successfully! Ready for questions.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            st.error(f\"An error occurred during processing: {e}\")\n",
    "            st.session_state.processing_done = False\n",
    "            # print(traceback.format_exc()) # For debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ccc8cf-d36e-4a70-9f83-4581a62c141c",
   "metadata": {},
   "source": [
    "2. Display Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd70a5ff-033d-4041-8214-983d3551f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use st.container() for better layout control if needed\n",
    "chat_container = st.container()\n",
    "with chat_container:\n",
    "    if st.session_state.processing_done:\n",
    "        for message in st.session_state.chat_history_display:\n",
    "            with st.chat_message(message.type): # 'human' or 'ai'\n",
    "                 st.markdown(message.content)\n",
    "                 # Display context only for AI messages where it was captured\n",
    "                 if isinstance(message, AIMessage) and hasattr(message, 'source_docs'):\n",
    "                     with st.expander(\"Show Retrieved Context Used\"):\n",
    "                        for doc in message.source_docs:\n",
    "                            # Try to get source metadata if available\n",
    "                            source = doc.metadata.get('source', 'Unknown chunk')\n",
    "                            st.markdown(f\"**Source:** `{source}`\")\n",
    "                            st.caption(doc.page_content)\n",
    "                            st.markdown(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a573d71-aa6d-4916-80d7-75b4ade0c47c",
   "metadata": {},
   "source": [
    "3. Chat Input Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8662be-f475-4dbe-9986-ddd96a631785",
   "metadata": {},
   "outputs": [],
   "source": [
    "if st.session_state.processing_done:\n",
    "    user_question = st.chat_input(\"Ask a question about the document...\")\n",
    "\n",
    "    if user_question:\n",
    "        if st.session_state.conversation_chain:\n",
    "            # Add user question to display history immediately\n",
    "            st.session_state.chat_history_display.append(HumanMessage(content=user_question))\n",
    "\n",
    "            # Display user message in chat container\n",
    "            with chat_container:\n",
    "                with st.chat_message(\"human\"):\n",
    "                    st.markdown(user_question)\n",
    "\n",
    "            # Process the question using the chain\n",
    "            with st.spinner(\"Thinking...\"):\n",
    "                try:\n",
    "                    # The chain uses its internal memory which includes previous turns\n",
    "                    # We pass the current question and the *internal* memory handles history\n",
    "                    result = st.session_state.conversation_chain({\n",
    "                        \"question\": user_question,\n",
    "                        # chat_history is implicitly handled by the memory object passed during chain creation\n",
    "                    })\n",
    "                    answer = result['answer']\n",
    "                    retrieved_docs = result['source_documents']\n",
    "\n",
    "                    # Update token counts (estimation)\n",
    "                    st.session_state.cumulative_input_tokens += count_tokens(user_question)\n",
    "                    st.session_state.cumulative_output_tokens += count_tokens(answer)\n",
    "                    update_token_display()\n",
    "\n",
    "                    # Create AI message with source docs attached for display\n",
    "                    ai_message = AIMessage(content=answer)\n",
    "                    ai_message.source_docs = retrieved_docs # Attach docs for the expander\n",
    "\n",
    "                    # Add AI response to display history\n",
    "                    st.session_state.chat_history_display.append(ai_message)\n",
    "\n",
    "                    # Rerun the script to update the chat display including the new AI message and context\n",
    "                    st.rerun()\n",
    "\n",
    "                except Exception as e:\n",
    "                    st.error(f\"An error occurred while getting the answer: {e}\")\n",
    "                    # Remove the user message we optimistically added if AI fails\n",
    "                    if st.session_state.chat_history_display and isinstance(st.session_state.chat_history_display[-1], HumanMessage):\n",
    "                       st.session_state.chat_history_display.pop()\n",
    "\n",
    "\n",
    "        else:\n",
    "            st.warning(\"Conversation chain not initialized. Please process a document first.\")\n",
    "elif not uploaded_file:\n",
    "    st.info(\"Please upload a PDF document using the sidebar to begin.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
